\documentclass[11pt]{article}
\addtolength{\topmargin}{-0.375in}
\addtolength{\oddsidemargin}{-0.35in}
\addtolength{\evensidemargin}{-0.35in}
\addtolength{\textheight}{0.5in}
\addtolength{\textwidth}{1.1in}
\addtolength{\parskip}{0.05in}
\renewcommand{\baselinestretch}{1.1}
\newcommand{\vsp}{\vspace{\baselineskip}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\gr}[1]{\ensuremath  {\frac{\dot {#1}}{#1}}}


\begin{document}

\noindent

\textbf{First just the time series model (i.e., not the panel models)}

\textbf{Models with yield or factors based on yield only}

\begin{enumerate}
\item No change forecast 
\begin{itemize}
\item I'm listing this first because I think we should also show tables of relative RSMEs and for these tables I think the relevant comparison should be relative to the no change forecast.    
\begin{itemize}
\item The first table could show all the RSMEs in absolute terms (as they are now shown).  
\item The second table could show just the RSME for the no change forecast and then relative RSMEs for all other approaches; that is, $\sqrt{\sum \epsilon^{2}_{alt.mod.}}/\sqrt{\sum \epsilon^{2}_{no.chg}}$.
\item The third table could show just the RSME for the no change forecast and then show Barbara's (relative) full sample $A$ measures, defined as $\sum \epsilon^{2}_{alt.mod.}-\sum \epsilon^{2}_{no.chg}$, just to set things up for later. From an ordering perspective Barbara's full sample $A$ measures should be the same as reporting relative RMSEs.
\end{itemize}
\item Additionally, it may also be worth having some charts reporting rolling window relative RMSEs and rolling window versions of Barbara's $A$ measure.  
\begin{itemize}
\item It would probably make sense to do this only for a selection of models (i.e., models that are different enough).  
\item Doing the rolling window versions of Barbara's $A$ measure would set things up later for doing the hand-wavy version of Barbara's decomposition. (Recall, that we can not do a proper version of Barbara's decomposition because she does it for DAR forecasts and while we could do DAR forecasts they do not really make sense when we want to do conditional forecasts.)  
\end{itemize}
\end{itemize}
\item Models with yields
\begin{enumerate}
\item Equally-weighted average (EWA) of 12 recursive autoregression (RAR) models that individual includes all 12 yields 
\item EWA of 3 RAR models that individually include the 3-month rate, 2-year yield, and 10-year yield
\item EWA of 2 RAR models that individually include the 3-month rate and 10-year yield
\end{enumerate}
\item Models with data-based factors ($level=10.year.yield$, $slope=10.year.yield–-3.month.rate$, and $curvature=2*2.year.yield–-3.month.rate–-10.year.yield$)
\begin{enumerate}
\item RAR model that jointly includes the 3 data-based factors
\item EWA of 3 RAR models that individually include the 3 data-based factors
\end{enumerate}
\item Models with PCA-based factors (1st PC, 2nd PC, and 3rd PC)
\begin{enumerate}
\item RAR model that jointly includes the first 3 PCA-based factors
\item EWA of 3 RAR models that individually include the first 3 PCA-based factors
\end{enumerate}
\item Models with Nelson-Siegel model-based factors (level, slope, and curvature)
\begin{enumerate}
\item RAR model that jointly includes the 3 Nelson-Siegel-based factors
\item EWA of 3 RAR models that individually include the 3 Nelson-Siegel-based factors
\end{enumerate}
\item Models with PLS-based factors
\begin{enumerate}
\item RAR model that jointly includes the first 3 PLS-based factors
\item EWA of 3 RAR models that individually include the first 3 PLS-based factors
\item RAR model that includes the first PLS-based factor
\end{enumerate}
\item Should we include the system approaches that we have already done
\begin{enumerate}
\item The VAR model with NIMs and the data-based factors
\item The VAR model with NIMs and the Nelson-Siegel model-based factors
\end{enumerate}
\item Bayesian model averaging – should we pursue?
\end{enumerate}

\textbf{Models with yield or factors based on yield and banking variables}

\begin{itemize}
\item Models listed above plus a measure of banking competition (the shadow banking sector's share of intermediation)
\item Models listed above plus a measure of interest rate volatility (i.e., the MOVE index or the volatility measures from the banking literature)
\item I think that ultimately these additions to the model fail to improve forecast performance so these results are really just for completeness and in recognition of the previous banking literature on NIMs.
\end{itemize}

\textbf{Model estimation period}

\begin{itemize}
\item Expanding windows [this is what we have been doing]
\item Fixed rolling windows
\end{itemize}

\textbf{Type of forecasting model}

\begin{itemize}
\item Recursive autoregression (RAR) forecasting models [this is what we have been using]
\item Direct autoregression (DAR) forecasting models
\end{itemize}

\textbf{Forecast evaluation period}

\begin{itemize}
\item Out-of-sample [this is what we have been doing] 
\item In-sample
\end{itemize}

\textbf{Barbara's method for comparing two models}

\begin{itemize}
\item Barbara's method applies to DAR forecasts; that is, $y_{t+h}=\alpha_{0}x_{t}+\alpha_{1}x_{t-1}+\cdots+\epsilon^{\alpha}_{t}$ and $y_{t+h}=\gamma_{0}z_{t}+\gamma_{1}z_{t-1}+\cdots+\epsilon^{\gamma}_{t}$, and not forecasts generated recursively (i.e., RAR forecasts).  Most of the paper works with rolling window estimation periods, though in the last paragraph of section~3 she says that everything goes through with expanding estimation periods and the discussion is in the appendix (though it sounds like there is some adjustment needed for variances when doing the decomposition).
\item Models A and G can be estimated for different step-ahead equations and over fixed rolling windows and used to forecast out-of-sample.  This will imply for every period and every $h$-step ahead forecast a squared forecast error $\left(\overrightarrow{\eta}^{\alpha}_{t+h}\right)^{2}$ and
 $\left(\overrightarrow{\eta}^{\gamma}_{t+h}\right)^{2}$.  (I'm using ``over-arrows'' to denote out-of-sample forecasts.  This is my notation, not Barbara's. I realize that over-arrows typically mean vectors but I find it more intuitive to denote out-of-sample forecasts with an arrow and in-sample with a bar.)
\begin{itemize}
\item For $1$-step ahead we have $\left\{\left(\overrightarrow{\eta}^{\alpha}_{R+i+1|R+i}\right)^{2}\right\}_{i=0}^{T-R-1}$ and $\left\{\left(\overrightarrow{\eta}^{\gamma}_{R+i+1|R+i}\right)^{2}\right\}_{i=0}^{T-R-1}$, where the rolling window estimation period \underline{and data used to generate the forecast}
ends at $R, R+1, \dots, T-1$.
\item For $2$-step ahead we have $\left\{\left(\overrightarrow{\eta}^{\alpha}_{R+i+2|R+i}\right)^{2}\right\}_{i=0}^{T-R-2}$ and $\left\{\left(\overrightarrow{\eta}^{\gamma}_{R+i+2||R+i}\right)^{2}\right\}_{i=0}^{T-R-2}$, where the rolling window estimation period \underline{and data used to generate the forecast} ends at $R, R+1, \dots, T-2$.
\item $\cdots$
\item $\cdots$
\item For $h$-step ahead we have $\left\{\left(\overrightarrow{\eta}^{\alpha}_{R+i+h||R+i}\right)^{2}\right\}_{i=0}^{T-R-h}$ and $\left\{\left(\overrightarrow{\eta}^{\gamma}_{R+i+h||R+i}\right)^{2}\right\}_{i=0}^{T-R-h}$, where the rolling window estimation period \underline{and data used to generate the forecast} ends at $R, R+1, \dots, T-h$.
\item Note there is a different model estimated for each $1$-step, $2$-step, ..., $h$-step ahead forecast and a different model estimated for each date.  
\end{itemize}
\item One can then take the difference of these squared forecast errors $\left(\overrightarrow{\eta}^{\alpha}_{t+h}\right)^{2}$ and
 $\left(\overrightarrow{\eta}^{\gamma}_{t+h}\right)^{2}$, which I'll call $\overrightarrow{\mathcal{RL}}_{t+h}$ for ``rolling loss.''
\begin{itemize}
\item For $1$-step ahead: $\left\{\overrightarrow{\mathcal{RL}}_{R+i+1|R+i}\right\}_{i=0}^{T-R-1}\!\!\!\!=\left\{\left(\overrightarrow{\eta}^{\alpha}_{R+i+1|R+i}\right)^{2}\!\!-\left(\overrightarrow{\eta}^{\gamma}_{R+i+1|R+i}\right)^{2}\right\}_{i=0}^{T-R-1}$
\item For $2$-step ahead: $\left\{\overrightarrow{\mathcal{RL}}_{R+i+2|R+i}\right\}_{i=0}^{T-R-2}\!\!\!\!=\left\{\left(\overrightarrow{\eta}^{\alpha}_{R+i+2|R+i}\right)^{2}\!\!-\left(\overrightarrow{\eta}^{\gamma}_{R+i+2|R+i}\right)^{2}\right\}_{i=0}^{T-R-2}$
\item $\cdots$
\item $\cdots$
\item For $h$-step ahead: $\left\{\overrightarrow{\mathcal{RL}}_{R+i+h|R+i}\right\}_{i=0}^{T-R-h}\!\!\!\!=\left\{\left(\overrightarrow{\eta}^{\alpha}_{R+i+h|R+i}\right)^{2}\!\!-\left(\overrightarrow{\eta}^{\gamma}_{R+i+h|R+i}\right)^{2}\right\}_{i=0}^{T-R-h}$
\end{itemize}
\item So for any date after $R+h$ there are $h$ rolling loss $\overrightarrow{\mathcal{RL}}$ observations, corresponding to different step ahead forecasts, for date $R+h-1$ there are $h-1$ rolling loss $\overrightarrow{\mathcal{RL}}$ observations, for date $R+h-2$ there are $h-2$ rolling loss $\overrightarrow{\mathcal{RL}}$ observations, and so on until $R+1$ where there is just one rolling loss $\overrightarrow{\mathcal{RL}}$ observation.
\item Models A and G can be estimated over fixed rolling windows and used to forecast in-sample.  Note that whereas when one forecasts out-of-sample there is for each estimated model only 1 $1$-step ahead forecast, only 1 $2$-step ahead forecast, ..., and only 1 $h$-step ahead forecast, when one forecasts in-sample there is for each estimated fixed rolling window of length $R$, $R-1$ $1$-step ahead forecasts (for different quarters), $R-2$ $2$-step ahead forecasts (for different quarter), ..., and $R-h$ $h$-step ahead forecasts (for different quarters).  When looking at in-sample forecasts errors Barbara only looks at the \emph{very last} $1$-step, \emph{very last} $2$-step, ..., \emph{very last} $h$-step ahead forecast for that model.  
\item Models A and G estimated over fixed rolling windows and used to forecast in-sample will for every period and every $h$-step ahead forecast imply a squared forecast error for the last period of the estimation period $\left(\overline{\eta}^{\alpha}_{t}\right)^{2}$ and
 $\left(\overline{\eta}^{\gamma}_{t}\right)^{2}$.  (I'm using ``over bars'' to denote in-sample forecasts.  This is my notation, not Barbara's.)
\begin{itemize}
\item For $1$-step ahead we have $\left\{\left(\overline{\eta}^{\alpha}_{R+i|R+i-1}\right)^{2}\right\}_{i=0}^{T-R}$ and $\left\{\left(\overline{\eta}^{\gamma}_{R+i|R+i-1}\right)^{2}\right\}_{i=0}^{T-R}$, where in each case the rolling window estimation period
ends at $R, R+1, \dots, T$ but the data used to generate the forecast ends in $R-1, R, \dots, T-1$.
\item For $2$-step ahead we have $\left\{\left(\overline{\eta}^{\alpha}_{R+i|R+i-2}\right)^{2}\right\}_{i=0}^{T-R}$ and $\left\{\left(\overline{\eta}^{\gamma}_{R+i|R+i-2}\right)^{2}\right\}_{i=0}^{T-R}$, where in each case the rolling window estimation period
ends at $R, R+1, \dots, T$ but the data used to generate the forecast ends in $R-2, R-1, \dots, T-2$.
\item $\cdots$
\item $\cdots$
\item For $h$-step ahead we have $\left\{\left(\overline{\eta}^{\alpha}_{R+i|R+i-h}\right)^{2}\right\}_{i=0}^{T-R}$ and $\left\{\left(\overline{\eta}^{\gamma}_{R+i|R+i-h}\right)^{2}\right\}_{i=0}^{T-R}$, where in each case the rolling window estimation period
ends at $R, R+1, \dots, T$ but the data used to generate the forecast ends in $R-h, R-h+1, \dots, T-h$.
\end{itemize}
\item One can then take the difference of these squared forecast errors $\left(\overline{\eta}^{\alpha}_{t}\right)^{2}$ and
 $\left(\overline{\eta}^{\gamma}_{t}\right)^{2}$, which I'll call $\overline{\mathcal{RL}}_{t+h}$ for ``rolling loss.''
\begin{itemize}
\item For $1$-step ahead: $\left\{\overline{\mathcal{RL}}_{R+i|R+i-1}\right\}_{i=0}^{T-R-1}\!\!\!\!=\left\{\left(\overline{\eta}^{\alpha}_{R+i|R+i-1}\right)^{2}\!\!-\left(\overline{\eta}^{\gamma}_{R+i|R+i-1}\right)^{2}\right\}_{i=0}^{T-R-1}$
\item For $2$-step ahead: $\left\{\overline{\mathcal{RL}}_{R+i|R+i-2}\right\}_{i=0}^{T-R-2}\!\!\!\!=\left\{\left(\overline{\eta}^{\alpha}_{R+i|R+i-2}\right)^{2}\!\!-\left(\overline{\eta}^{\gamma}_{R+i|R+i-2}\right)^{2}\right\}_{i=0}^{T-R-2}$
\item $\cdots$
\item $\cdots$
\item For $h$-step ahead: $\left\{\overline{\mathcal{RL}}_{R+i|R+i-h}\right\}_{i=0}^{T-R-h}\!\!\!\!=\left\{\left(\overline{\eta}^{\alpha}_{R+i|R+i-h}\right)^{2}\!\!-\left(\overline{\eta}^{\gamma}_{R+i|R+i-h}\right)^{2}\right\}_{i=0}^{T-R-h}$
\end{itemize}
\item Then one starts regressing different losses on each other.
\begin{itemize}
\item For the $1$-step ahead forecast regress $\left\{\overrightarrow{\mathcal{RL}}_{R+i+1|R+i}\right\}_{i=0}^{T-R-1}$ on $\left\{\overline{\mathcal{RL}}_{R+i|R+i-1}\right\}_{i=0}^{T-R-1}$.  The models being used to generate these losses are estimated on rolling windows that end on $R, R+1, \cdots, T$.
\item For the $2$-step ahead forecast regress $\left\{\overrightarrow{\mathcal{RL}}_{R+i+2|R+i}\right\}_{i=0}^{T-R-2}$ on $\left\{\overline{\mathcal{RL}}_{R+i|R+i-2}\right\}_{i=0}^{T-R-2}$.  The models being used to generate these losses are estimated on rolling windows that end on $R, R+1, \cdots, T$.
\item $\cdots$
\item $\cdots$
\item For the $h$-step ahead forecast regress $\left\{\overrightarrow{\mathcal{RL}}_{R+i+h|R+i}\right\}_{i=0}^{T-R-h}$ on $\left\{\overline{\mathcal{RL}}_{R+i|R+i-h}\right\}_{i=0}^{T-R-h}$.  The models being used to generate these losses are estimated on rolling windows that end on $R, R+1, \cdots, T$.
\end{itemize}
\item The regressions are:
\begin{itemize}
\item $1$-step: $\overrightarrow{\mathcal{RL}}_{R+i+1|R+i}=\widehat{\beta}_{1}\overline{\mathcal{RL}}_{R+i|R+i-1}+\widehat{u}_{R+i+1}$, where $i=0, 1, \cdots, T-R-1$.
\item $2$-step: $\overrightarrow{\mathcal{RL}}_{R+i+2|R+i}=\widehat{\beta}_{2}\overline{\mathcal{RL}}_{R+i|R+i-2}+\widehat{u}_{R+i+2}$, where $i=0, 1, \cdots, T-R-2$.
\item $\cdots$
\item $\cdots$
\item $h$-step: $\overrightarrow{\mathcal{RL}}_{R+i+h|R+i}=\widehat{\beta}_{h}\overline{\mathcal{RL}}_{R+i|R+i-h}+\widehat{u}_{R+i+h}$, where $i=0, 1, \cdots, T-R-h$.
\item It may be worth noting that every out-of-sample-based loss being regressed on a corresponding in-sample-based loss uses exactly the same two versions -- where versions refer to the estimation period -- of the A and G model.  That is the models are all estimated on rolling windows ending $R, R+1, \cdots, T-s$ but the in-sample forecasts use this estimated model but condition on earlier data for period $R-s$, $R+1-s$, $\cdots$, $T-s-s$ (where $s=1,2,\cdots,h$) in generating the forecasts for periods $R, R+1, \cdots, T$ while the out-of-sample forecasts use this estimated model, condition on $R$, $R+1$, $\cdots$, $T-s$ data and project out  $R+s$, $R+1+s$, $\cdots$, $T$ (where $s=1,2,\cdots,h$).
\end{itemize}
\item From the regressions we can calculate:
\begin{itemize}
\item $1$-step: 
\begin{eqnarray}
\underbrace{\frac{1}{T\!-\!R}\!\!\sum_{i=0}^{T-R-1}\!\!\overrightarrow{\mathcal{RL}}_{R+i+1|R+i}}_{=A_{1,T-R}}
\!\!\!\!&=&\!\!\!\!\underbrace{\widehat{\beta}_{1}\cdot\frac{1}{T\!-\!R}\!\!\sum_{i=0}^{T-R-1}\!\!\overline{\mathcal{RL}}_{R+i|R+i-1}}_{=B_{1,T-R}} 
+\underbrace{\frac{1}{T\!-\!R}\!\!\sum_{i=0}^{T-R-1}\!\!\widehat{u}_{R+i+1}}_{=U_{1,T-R}}, \nonumber
\end{eqnarray}
\item $2$-step: 
\begin{eqnarray}
\underbrace{\frac{1}{T\!-\!R\!-\!1}\!\!\sum_{i=0}^{T-R-2}\!\!\overrightarrow{\mathcal{RL}}_{R+i+2|R+i}}_{=A_{2,T-R-1}}
\!\!\!\!&=&\!\!\!\!\underbrace{\widehat{\beta}_{2}\cdot\frac{1}{T\!-\!R\!-\!1}\!\!\sum_{i=0}^{T-R-2}\!\!\overline{\mathcal{RL}}_{R+i|R+i-2}}_{=B_{2,T-R-1}} 
+\underbrace{\frac{1}{T\!-\!R\!-\!1}\!\!\sum_{i=0}^{T-R-2}\!\!\widehat{u}_{R+i+2}}_{=U_{2,T-R-1}}, \nonumber 
\end{eqnarray}
\item $\cdots$
\item $\cdots$
\item $h$-step: 
\begin{eqnarray}
\underbrace{\frac{1}{T\!-\!R\!-\!(h\!-\!1)}\!\!\!\!\sum_{i=0}^{T-R-h}\!\!\!\!\overrightarrow{\mathcal{RL}}_{R+i+h|R+i}}_{=A_{h,T-R-(h-1)}}
\!\!\!\!&=&\!\!\!\!\underbrace{\widehat{\beta}_{h}\cdot\frac{1}{T\!-\!R\!-\!(h\!-\!1)}\!\!\!\!\sum_{i=0}^{T-R-h}\!\!\!\!\overline{\mathcal{RL}}_{R+i|R+i-h}}_{=B_{h,T-R-(h-1)}} \ \ \ \   \ \ \ \    \ \ \ \   \ \ 
\nonumber \\
\!\!\!\!&+&\!\!\!\!
\underbrace{\frac{1}{T\!-\!R\!-\!(h\!-\!1)}\!\!\!\!\sum_{i=0}^{T-R-h}\!\!\!\!\widehat{u}_{R+i+h}}_{=U_{h,T-R-(h-1)}}, \nonumber
\end{eqnarray}
\end{itemize}
\item The interpretation of the $A$, $B$, and $U$ terms are as follows.
\begin{itemize}
\item $A_{1,T\!-\!R}$, $A_{2,T\!-\!R\!-\!1}$, ..., $A_{h,T\!-\!R\!-\!\!(h\!-\!1)}$ measures the two models' relative out-of-sample forecasting ability \underline{on average} over the \underline{full} out-of-sample forecast evaluation period.
\item $B_{1,T\!-\!R}$, $B_{2,T\!-\!R\!-\!1}$, ..., $B_{h,T\!-\!R\!-\!\!(h\!-\!1)}$ are proportional to the two models' relative in-sample forecasting ability \underline{on average} over the \underline{full} in-sample forecast evaluation period.  This represents the part of the two models' out-of-sample relative forecasting ability that is reflected in in-sample forecasting ability.  When the $B$ terms have the same sign as their corresponding $A$ term it means that in-sample relative forecast performance has predictive content for out-of-sample relative performance.  If they have the opposite sign, there is predictive content but it is misleading.
\item $U_{1,T\!-\!R}$, $U_{2,T\!-\!R\!-\!1}$, ..., $U_{h,T\!-\!R\!-\!\!(h\!-\!1)}$ represents the part of the two models' out-of-sample relative forecasting ability that is \underline{not} reflected in in-sample forecasting ability. 
\end{itemize}
\item Barbara also wanted to look at how the two models' relative out-of-sample forecasting ability varies across the full out-of-sample forecast evaluation period so they thought about rolling windows for:
\begin{eqnarray}
A_{1,T\!-\!R}\!\!\!\!&=&\!\!\!\!\frac{1}{T\!-\!R}\!\!\sum_{i=0}^{T-R-1}\!\!\overrightarrow{\mathcal{RL}}_{R+i+1|R+i} \nonumber \\
A_{2,T\!-\!R\!-\!1}\!\!\!\!&=&\!\!\!\!\frac{1}{T\!-\!R\!-\!1}\!\!\sum_{i=0}^{T-R-2}\!\!\overrightarrow{\mathcal{RL}}_{R+i+2|R+i} \nonumber \\
\!\!\!\!&&\!\!\!\!\cdots  \nonumber \\
\!\!\!\!&&\!\!\!\!\cdots  \nonumber \\
A_{h,T\!-\!R\!-\!\!(h\!-\!1)}\!\!\!\!&=&\!\!\!\!\frac{1}{T\!-\!R\!-\!(h\!-\!1)}\!\!\sum_{i=0}^{T-R-h}\!\!\overrightarrow{\mathcal{RL}}_{R+i+h|R+i}. \nonumber
\end{eqnarray}
\item If the moving windows are of length $m$ then:
\begin{itemize}
\item For $A_{1,T\!-\!R}$ there are $T-R-m+1$ possible windows, where the windows are: 
\[\left\{\overrightarrow{\mathcal{RL}}_{R+i+1+0|R+i+0}\ \ , \ \  . \ \  . \ \  . \ \ ,\ \ \overrightarrow{\mathcal{RL}}_{R+i+1+(m-1)|R+i+(m-1)}\right\}_{i=0}^{T-R-1+1-m}\] 
and so the rolling window relative out-of-sample forecast performance terms would be:
\[\frac{1}{m}\sum_{j=0}^{m-1}\overrightarrow{\mathcal{RL}}_{R+i+1+j|R+i+j}\ \ \ \mathrm{for} \ \ \ i=0,1,\cdots,T\!-\!R\!-\!1\!+\!1\!-\!m. \]
\item For $A_{2,T\!-\!R\!-\!1}$ there are $T-R-m$ possible windows, where the windows are: 
\[\left\{\overrightarrow{\mathcal{RL}}_{R+i+2+0|R+i+0}\ \ , \ \  . \ \  . \ \  . \ \ ,\ \ \overrightarrow{\mathcal{RL}}_{R+i+2+(m-1)|R+i+(m-1)}\right\}_{i=0}^{T-R-2+1-m}\] 
and so the rolling window relative out-of-sample forecast performance terms would be:
\[\frac{1}{m}\sum_{j=0}^{m-1}\overrightarrow{\mathcal{RL}}_{R+i+2+j|R+i+j}\ \ \ \mathrm{for} \ \ \ i=0,1,\cdots,T\!-\!R\!-\!2\!+\!1\!-\!m. \]
\item And so on for all the other steps until we get to the $h$-th step
\item For $A_{h,T\!-\!R\!-\!(h-1)}$ there are $T-R-m-(h-1)$ possible windows, where the windows are: 
\[\left\{\overrightarrow{\mathcal{RL}}_{R+i+h+0|R+i+0}\ \ , \ \  . \ \  . \ \  . \ \ ,\ \ \overrightarrow{\mathcal{RL}}_{R+i+h+(m-1)|R+i+(m-1)}\right\}_{i=0}^{T-R-h+1-m}\] 
and so the rolling window relative out-of-sample forecast performance terms would be:
\[\frac{1}{m}\sum_{j=0}^{m-1}\overrightarrow{\mathcal{RL}}_{R+i+h+j|R+i+j}\ \ \ \mathrm{for} \ \ \ i=0,1,\cdots,T\!-\!R\!-\!h\!+\!1\!-\!m. \]
\end{itemize}
\item So then it is the rolling window relative out-of-sample forecast performances -- that is, $\frac{1}{m}\sum_{j=0}^{m-1}\overrightarrow{\mathcal{RL}}_{R+i+1+j|R+i+j}$, $\frac{1}{m}\sum_{j=0}^{m-1}\overrightarrow{\mathcal{RL}}_{R+i+2+j|R+i+j}$, . . . , $\frac{1}{m}\sum_{j=0}^{m-1}\overrightarrow{\mathcal{RL}}_{R+i+h+j|R+i+j}$ -- that one wants to decompose.  These are done as follows.
\begin{eqnarray}
\frac{1}{m}\sum_{j=0}^{m-1}\overrightarrow{\mathcal{RL}}_{R+i+1+j|R+i+j}\!\!\!\!&=&\!\!\!\!\underbrace{\frac{1}{m}\sum_{j=0}^{m-1}\overrightarrow{\mathcal{RL}}_{R+i+1+j|R+i+j}-\underbrace{\frac{1}{T\!-\!R}\!\!\sum_{i=0}^{T-R-1}\!\!\overrightarrow{\mathcal{RL}}_{R+i+1|R+i}}_{=A_{1,T\!-\!R}}}_{=A_{i,1,T-R}} \nonumber \\
\!\!\!\!&+&\!\!\!\!B_{1,T-R}+U_{1,T-R} \ \ \ \ \   \ \ \ \ \   \ \  \mathrm{for} \ i=0,1,\cdots,T\!-\!R\!-\!1\!+\!1\!-\!m. \nonumber
\end{eqnarray}
\begin{eqnarray}
\frac{1}{m}\sum_{j=0}^{m-1}\overrightarrow{\mathcal{RL}}_{R+i+2+j|R+i+j}\!\!\!\!&=&\!\!\!\!\underbrace{\frac{1}{m}\sum_{j=0}^{m-1}\overrightarrow{\mathcal{RL}}_{R+i+2+j|R+i+j}-\underbrace{\frac{1}{T\!-\!R\!-\!1}\!\!\sum_{i=0}^{T-R-2}\!\!\overrightarrow{\mathcal{RL}}_{R+i+2|R+i}}_{=A_{2,T\!-\!R\!-\!1}}}_{=A_{i,2,T-R-1}} \nonumber \\
\!\!\!\!&+&\!\!\!\!B_{2,T-R-1}+U_{2,T-R-1} \ \ \ \ \   \ \  \mathrm{for} \ i=0,1,\cdots,T\!-\!R\!-\!2\!+\!1\!-\!m. \nonumber
\end{eqnarray}
\[ \cdots \]
\[ \cdots \]
\begin{eqnarray}
\frac{1}{m}\sum_{j=0}^{m-1}\overrightarrow{\mathcal{RL}}_{R+i+h+j|R+i+j}\!\!\!\!&=&\!\!\!\!\underbrace{\frac{1}{m}\sum_{j=0}^{m-1}\overrightarrow{\mathcal{RL}}_{R+i+h+j|R+i+j}-\underbrace{\frac{1}{T\!-\!R\!-\!(h\!-\!1)}\!\!\sum_{i=0}^{T-R-h}\!\!\overrightarrow{\mathcal{RL}}_{R+i+h|R+i}}_{=A_{h,T\!-\!R-(h-1)}}}_{=A_{i,h,T-R-(h-1)}} \nonumber \\
\!\!\!\!&+&\!\!\!\!B_{h,T-R-(h-1)}+U_{h,T-R-(h-1)} \ \ \ \ \   \mathrm{for} \ i=0,1,\cdots,T\!-\!R\!-\!h\!+\!1\!-\!m. \nonumber
\end{eqnarray}
In summary we have
\begin{eqnarray}
\frac{1}{m}\sum_{j=0}^{m}\overrightarrow{\mathcal{RL}}_{R+i+1+j|R+i+j}\!\!\!\!&=&\!\!\!\!A_{i,1,T-R}\!+\!B_{1,T-R}\!+\!U_{1,T-R} \ \ \ \ \    \ \ \ \ \  \mathrm{for} \ i=0,1,\cdots,T\!-\!R\!-\!1\!+\!1\!-\!m\nonumber \\
\frac{1}{m}\sum_{j=0}^{m}\overrightarrow{\mathcal{RL}}_{R+i+2+j|R+i+j}\!\!\!\!&=&\!\!\!\!A_{i,2,T-R-1}\!+\!B_{2,T-R-1}\!+\!U_{2,T-R-1} \ \mathrm{for} \ i=0,1,\cdots,T\!-\!R\!-\!2\!+\!1\!-\!m\nonumber \\
\cdots \nonumber \\
\cdots \nonumber \\
\frac{1}{m}\sum_{j=0}^{m}\overrightarrow{\mathcal{RL}}_{R+i+h+j|R+i+j}\!\!\!\!&=&\!\!\!\!A_{i,h,T-R-(h-1)}\!+\!B_{h,T-R-(h-1)}\!+\!U_{h,T-R-(h-1)}\ \nonumber \\
&& \ \ \ \    \ \ \ \   \ \ \ \    \ \ \ \    \ \ \ \    \ \ \ \   \ \ \ \    \ \ \ \   \ \ \ \   \ \ \ \    \ \ 
\mathrm{for} \ i=0,1,\cdots,T\!-\!R\!-\!h\!+\!1\!-\!m \nonumber
\end{eqnarray}
\item So basically one calculates the rolling window relative out-of-sample forecast performances -- that is, $\frac{1}{m}\sum_{j=0}^{m-1}\overrightarrow{\mathcal{RL}}_{R+i+1+j|R+i+j}$, $\frac{1}{m}\sum_{j=0}^{m-1}\overrightarrow{\mathcal{RL}}_{R+i+2+j|R+i+j}$, . . . , $\frac{1}{m}\sum_{j=0}^{m-1}\overrightarrow{\mathcal{RL}}_{R+i+h+j|R+i+j}$ -- and tries to understand them in terms of the above components. 
\begin{itemize}
\item  I think what is interesting about looking at these is that the relative out-of-sample forecasting performance of the two models could change over time.  That is, over some windows one of the two models may forecast better where as over other windows the other of the two models may forecast better. 
\end{itemize}
\item The next step is to makes test statistics out of the $\left\{\left\{A_{i,s,T-R}\right\}_{i=0}^{T-R-s+1-m},B_{s,T-R},U_{s,T-R}\right\}_{s=1}^{h}$
terms calculated above; though a few more terms are needed to calculate these. [Note that we are back to calculating things over the full forecast sample.]  Let:
\begin{eqnarray}
\widehat{rl}_{R+j+1}\!\!\!\!&=&\!\!\!\! \left(\begin{array}{c} \overrightarrow{\mathcal{RL}}_{R+j+1|R+j} \\ \overline{\mathcal{RL}}_{R+j|R+j-1}\cdot\overrightarrow{\mathcal{RL}}_{R+j+1|R+j} \end{array} \right),
\textrm{where} \ \ j=0, 1, \cdots, T-R-1
\nonumber \\
\widehat{rl}_{R+j+2}\!\!\!\!&=&\!\!\!\! \left(\begin{array}{c} \overrightarrow{\mathcal{RL}}_{R+j+2|R+j} \\ \overline{\mathcal{RL}}_{R+j|R+j-2}\cdot\overrightarrow{\mathcal{RL}}_{R+j+2|R+j} \end{array} \right),
\textrm{where} \ \ j=0, 1, \cdots, T-R-2
\nonumber \\
\cdots
\nonumber \\
\cdots
\nonumber \\
\widehat{rl}_{R+j+h}\!\!\!\!&=&\!\!\!\! \left(\begin{array}{c} \overrightarrow{\mathcal{RL}}_{R+j+h|R+j} \\ \overline{\mathcal{RL}}_{R+j|R+j-h}\cdot\overrightarrow{\mathcal{RL}}_{R+j+h|R+j} \end{array} \right),
\textrm{where} \ \ j=0, 1, \cdots, T-R-h
\nonumber
\end{eqnarray}
\item Barbara then defines (where I assumed ``$d$" denotes de-meaned, again over the full forecast sample):
\begin{eqnarray}
\underbrace{\widehat{rl}^{d}_{R+j+1}}_{(2x1)}\!\!\!\!&=&\!\!\!\!\widehat{rl}_{R+j+1}-\frac{1}{T-R-1+1}\sum_{i=0}^{T-R-1}\widehat{rl}_{R+i+1}, \ \textrm{where} \ \ j=0, 1, \cdots, T-R-1 \nonumber \\
\underbrace{\widehat{rl}^{d}_{R+j+2}}_{(2x1)}\!\!\!\!&=&\!\!\!\!\widehat{rl}_{R+j+2}-\frac{1}{T-R-2+1}\sum_{i=0}^{T-R-2}\widehat{rl}_{R+i+2}, \ \textrm{where} \ \ j=0, 1, \cdots, T-R-2\nonumber \\
\cdots
\nonumber \\
\cdots
\nonumber \\
\underbrace{\widehat{rl}^{d}_{R+j+h}}_{(2x1)}\!\!\!\!&=&\!\!\!\!\widehat{rl}_{R+j+h}-\frac{1}{T-R-h+1}\sum_{i=0}^{T-R-h}\widehat{rl}_{R+i+h}, \ \textrm{where} \ \ j=0, 1, \cdots, T-R-h
\nonumber
\end{eqnarray}
\item *****************************************************************************
\item There seem to be parallels with this an Newey-West (1987), as Barbara alludes to.  See section 15.4.3 (on pages 422 and 423) of Greene. 
\item For the $1$-step ahead forecast Barbara calculates
\[ \widehat{\Omega}_{roll,1}=\frac{1}{T-R}\sum_{i=-(q(T-R)-1)}^{+(q(T-R)-1)}\left(1-\left|\frac{i}{q(T-R)}\right|\right)\sum_{j=0}^{T-R-1}(\widehat{rl}^{d}_{R+j+1})(\widehat{rl}^{d}_{R+j+1})^{\prime}
\]
(So if $q=4$ then $t$ would be $\{-3,-2,-1,0,1,2,3\}$ and then the $\left(1-\left|\frac{i}{q(T-R)}\right|\right)$ term would be $\{1-\frac{3}{4},1-\frac{2}{4},1-\frac{1}{4},1-\frac{0}{4},1-\frac{1}{4},1-\frac{2}{4},1-\frac{2}{4}\}$ or $\{\frac{1}{4},\frac{2}{4},\frac{3}{4},1,\frac{3}{4},\frac{2}{4},\frac{1}{4}\}$  The thing that I don't understand is that there doesn't seem to be any interaction between the first and second summations.  That is, $i$ doesn't enter the $\sum_{j=0}^{T-R-1}(\widehat{rl}^{d}_{R+j+1})(\widehat{rl}^{d}_{R+j+1})^{\prime}$ term.) 
\item For the $2$-step ahead forecast Barbara calculates (I think)
\[ \widehat{\Omega}_{roll,2}=\frac{1}{T-R-1}\sum_{i=-(q(T-R-1)-1)}^{+(q(T-R-1)-1)}\left(1-\left|\frac{i}{q(T-R-1)}\right|\right)\sum_{j=0}^{T-R-2}(\widehat{rl}^{d}_{R+j+2})(\widehat{rl}^{d}_{R+j+2})^{\prime}
\]
\item ...
\item ...
\item For the $h$-step ahead forecast Barbara calculates (I think)
\[ \widehat{\Omega}_{roll,h}=\!\!  \frac{1}{T\!-\!R\!-\!(h\!-\!1)}\!\!\sum_{i=-(q(T-R-(h-1))-1)}^{+(q(T-R-(h-1))-1)}\!\!\left(1\!-\!\left|\frac{i}{q(T\!-\!R\!-\!(h\!-\!1))}\right|\right)\!\!\sum_{j=0}^{T-R-h}\!\!\!(\widehat{rl}^{d}_{R+j+2})(\widehat{rl}^{d}_{R+j+2})^{\prime}
\]
\item *****************************************************************************
\item For the $1$-step ahead forecast one takes:
\begin{eqnarray}&&\!\!\!\!\widehat{\sigma}^{2}_{A,1}=\widehat{\Omega}_{(1,1),roll,1},\ \widehat{\sigma}^{2}_{B,1}=\Phi^{2}_{T-R}\widehat{\Omega}_{(2,2),roll,1}\ \mathrm{and} \ \widehat{\sigma}^{2}_{U,1}=\widehat{\sigma}^{2}_{A,1}-\widehat{\sigma}^{2}_{B,1}, \nonumber \\
&&\!\!\!\!\mathrm{where}\ \Phi^{2}_{T-R}=\left(\frac{1}{T-R}\sum_{i=0}^{T-R-1} \overline{\mathcal{RL}}_{R+i|R+i-1}\right) \left(\frac{1}{T-R}\sum_{i=0}^{T-R-1} \left(\overline{\mathcal{RL}}_{R+i|R+i-1}\right)^{2}\right)^{-1} \nonumber
\end{eqnarray}
\item For the $2$-step ahead forecast one takes:
\begin{eqnarray}&&\!\!\!\!\widehat{\sigma}^{2}_{A,2}=\widehat{\Omega}_{(1,1),roll,2},\ \widehat{\sigma}^{2}_{B,2}=\Phi^{2}_{T-R}\widehat{\Omega}_{(2,2),roll,2}\ \mathrm{and} \ \widehat{\sigma}^{2}_{U,2}=\widehat{\sigma}^{2}_{A,2}-\widehat{\sigma}^{2}_{B,2}, \nonumber \\
&&\!\!\!\!\mathrm{where}\ \Phi^{2}_{T-R-1}=\left(\frac{1}{T\!-\!R\!-\!1}\sum_{i=0}^{T-R-2} \overline{\mathcal{RL}}_{R+i|R+i-2}\right) \left(\frac{1}{T\!-\!R\!-\!1}\sum_{i=0}^{T-R-2} \left(\overline{\mathcal{RL}}_{R+i|R+i-2}\right)^{2}\right)^{-1} \nonumber
\end{eqnarray}
\item ...
\item ...
\item For the $h$-step ahead forecast one takes:
\begin{eqnarray}&&\!\!\!\!\widehat{\sigma}^{2}_{A,h}=\widehat{\Omega}_{(1,1),roll,h},\ \widehat{\sigma}^{2}_{B,h}=\Phi^{2}_{T-R}\widehat{\Omega}_{(2,2),roll,h}\ \mathrm{and} \ \widehat{\sigma}^{2}_{U,h}=\widehat{\sigma}^{2}_{A,h}-\widehat{\sigma}^{2}_{B,h}, \nonumber \\
&&\!\!\!\!\mathrm{where}\ \Phi^{2}_{T-R-(h-1)}=\left(\frac{1}{T\!-\!R\!-\!(h\!-\!1)}\sum_{i=0}^{T-R-h} \overline{\mathcal{RL}}_{R+i|R+i-h}\right) \nonumber \\ && \ \ \ \   \ \ \ \   \ \ \ \   \ \ \ \   \ \ \ \   \ \ \ \   \ \ \ \   \ \ \ \   \times\left(\frac{1}{T\!-\!R\!-\!(h\!-\!1)}\sum_{i=0}^{T-R-h}\left (\overline{\mathcal{RL}}_{R+i|R+i-h}\right)^{2}\right)^{-1} \ \ \ \   \ \ \ \    \ \ \ \   \nonumber
\end{eqnarray}
\item Then one calculates the test-statistics for seeing if $\left\{\left\{A_{i,s,T-R}\right\}_{i=0}^{T-R-s-m},B_{s,T-R},U_{s,T-R}\right\}_{s=1}^{h}$ terms are significant or not.  Barbara just does one example of this -- that is for one $h$-step ahead forecast -- and these are her equations in section~4.  I think one actually has to do things sligthly differently for $1$-step, $2$-step, ..., $h$-step ahead forecasts.
\item I think the statistics for the $1$-step ahead forecast will be:
\begin{eqnarray}
\Gamma^{(A)}_{1,T-R}\!\!\!\!&=&\!\!\!\!\sup_{i=0,1,\cdots,T-R-1+1-m}\left|\sqrt{T-R}\cdot \widehat{\sigma}^{-1}_{A,1}\cdot A_{i,1,T-R} \right|,
\nonumber \\
\Gamma^{(B)}_{1,T-R}\!\!\!\!&=&\!\!\!\! \sqrt{T-R}\cdot\widehat{\sigma}^{-1}_{B,1}\cdot B_{1,T-R}, \nonumber \\
\Gamma^{(U)}_{1,T-R}\!\!\!\!&=&\!\!\!\! \sqrt{T-R}\cdot\widehat{\sigma}^{-1}_{U,1}\cdot U_{1,T-R}, \nonumber
\end{eqnarray}
\item I think the statistics for the $2$-step ahead forecast will be:
\begin{eqnarray}
\Gamma^{(A)}_{2,T-R-1}\!\!\!\!&=&\!\!\!\!\sup_{i=0,1,\cdots,T-R-2+1-m}\left|\sqrt{T-R-1}\cdot \widehat{\sigma}^{-1}_{A,2}\cdot A_{i,2,T-R-1} \right|,
\nonumber \\
\Gamma^{(B)}_{2,T-R-1}\!\!\!\!&=&\!\!\!\! \sqrt{T-R-1}\cdot\widehat{\sigma}^{-1}_{B,2}\cdot B_{2,T-R-1}, \nonumber \\
\Gamma^{(U)}_{2,T-R-1}\!\!\!\!&=&\!\!\!\! \sqrt{T-R-1}\cdot\widehat{\sigma}^{-1}_{U,2}\cdot U_{2,T-R-1}, \nonumber
\end{eqnarray}
\item ...
\item ... 
\item I think the statistics for the $h$-step ahead forecast will be:
\begin{eqnarray}
\Gamma^{(A)}_{h,T-R-(h-1)}\!\!\!\!&=&\!\!\!\!\sup_{i=0,1,\cdots,T-R-h+1-m}\left|\sqrt{T-R-(h-1)}\cdot \widehat{\sigma}^{-1}_{A,h}\cdot A_{i,h,T-R-(h-1)} \right|,
\nonumber \\
\Gamma^{(B)}_{h,T-R-(h-1)}\!\!\!\!&=&\!\!\!\! \sqrt{T-R-(h-1)}\cdot\widehat{\sigma}^{-1}_{B,h}\cdot B_{h,T-R-(h-1)}, \nonumber \\
\Gamma^{(U)}_{h,T-R-(h-1)}\!\!\!\!&=&\!\!\!\! \sqrt{T-R-(h-1)}\cdot\widehat{\sigma}^{-1}_{U,h}\cdot U_{h,T-R-(h-1)}, \nonumber
\end{eqnarray}
\item The critical values for the test statistic associated with the $\Gamma^{(A)}$ terms has an important parameter, which doesn't appear with my notations.  This parameter is $\tau$, which is given by $\tau=[\lambda\cdot P_{s}]$ (recall $P_{s}=T-R-s+1$ with $s$ denoting step), where $\lambda\in[\mu,1]$.  The $\lambda$ parameter is actually the important one, this is what shows up in critical values for the test statistic associated with the $\Gamma^{(A)}$.  (See Table~1.)
\begin{itemize}
\item Barbara writes her $1$-step ahead forecast as: 
\[\frac{1}{m}\sum_{t=R+\tau-m}^{R+\tau-1}\widehat{L}_{t+1}=A_{\tau,P}\!+\!B_{P}\!+\!U_{P} \ \mathrm{where} \  \tau=\lambda\cdot P_{1} \ \mathrm{and} \  \lambda\in[\mu,1].
\]
\item I write the $1$-step ahead forecast as: 
\[\frac{1}{m}\sum_{j=0}^{m-1}\overrightarrow{\mathcal{RL}}_{R+i+1+j|R+i+j}=A_{1,i,T-R}+B_{1,T-R}+U_{1,T-R} \ \mathrm{where}\  i=0,1,\cdots,T-R-1+1-m.
\]
\item The $\tau$ parameter doesn't really affect the last window since $\lambda=1$ for the last window, which means that $\tau=P$. But given this Barbara's and my last windows are the same.
\begin{itemize}
\item For me, the last window goes from $j=0$ to $j=m-1$ for $i=T-R+1-1-m$, so from $\overrightarrow{\mathcal{RL}}_{T-m+1|T-m}$ to $\overrightarrow{\mathcal{RL}}_{T|T-1}$.
\item For Barbara, the $\tau$ for the last window is $\tau=P_{1}$ and it goes from $t=R+\tau-m=R+P_{1}-m$ (the 1st observation) to $t=R+\tau-1=R+P_{1}-1$ (the last observation), so from $\widehat{L}_{R+P_{1}-m+1}$ to $\widehat{L}_{R+P_{1}}$.
\item Since $T=R+P_{1}+1-1$ these are the same across the two notations.
\end{itemize}
\item We look at the observations for the 1st window. 
\begin{itemize}
\item For me, the 1st window goes from $j=0$ to $j=m-1$ for $i=0$, so from
$\overrightarrow{\mathcal{RL}}_{R+1|R}$ to $\overrightarrow{\mathcal{RL}}_{R+m|R+m-1}$.
\item For Barbara, the $\tau$ for the 1st window is $\tau=\lambda\cdot P_{1}=\mu\cdot P_{1}$ and it goes from $t=R+\tau-m=R+\mu\cdot P_{1}-m$ (the 1st observation) to $t=R+\tau-1=R+\mu\cdot P_{1}-1$ (the last observation), so from $\widehat{L}_{R+\mu\cdot P_{1}-m+1}$ to $\widehat{L}_{R+\mu\cdot P_{1}}$.
\item This means that $R+\mu\cdot P_{1}-m+1=R+1$ and $R+\mu\cdot P_{1}=R+m$ or $\underline{\mu=m/P_{1}=m/(T-R)}$.  The lowest value that Barbara reports in Table 1 is $0.1$ so we probably don't want the window that we use for looking at instability to be less than $10$~percent of the full forecast evaluation range, but this seems OK.
\end{itemize}
\item We look at the observations for the 2nd window. 
\begin{itemize}
\item For me, the 2nd window goes from $j=0$ to $j=m-1$ for $i=1$, so from
$\overrightarrow{\mathcal{RL}}_{R+2|R+1}$ to $\overrightarrow{\mathcal{RL}}_{R+m+1|R+m}$.
\item For Barbara, the $\tau$ for the 2nd window is $\tau=\lambda\cdot P_{1}=\widetilde{\mu}\cdot P_{1}$ and it goes from $t=R+\tau-m=R+\widetilde{\mu}\cdot P_{1}-m$ (the 1st observation) to $t=R+\tau-1=R+\widetilde{\mu}\cdot P_{1}-1$ (the last observation), so from $\widehat{L}_{R+\widetilde{\mu}\cdot P_{1}-m+1}$ to $\widehat{L}_{R+\widetilde{\mu}\cdot P_{1}}$.
\item This means that $R+\widetilde{\mu}\cdot P_{1}-m+1=R+2$ and $R+\widetilde{\mu}\cdot P_{1}=R+m+1$ or $\widetilde{\mu}=(m+1)/P_{1}=(m+1)/(T-R)$.  But I don't think this $\widetilde{\mu}$ is really needed for anything.
\end{itemize}
\item So I think Barbara's $\mu$'s for the $1$-step ahead forecasts are $\mu\in [\frac{m+0}{T-R},\frac{m+1}{T-R},\cdots,\frac{m+T-R-m}{T-R}]$ or $\mu=\frac{m+i}{T-R}$ for $i=0,1,\cdots,T-R-m$.  I think that only
the lowest $\mu$ that is relevant for the critical values in Table~1.
\end{itemize}
\item Now consider the $2$-step ahead forecasts.
\begin{itemize}
\item Barbara writes her $2$-step ahead forecast as: 
\[\frac{1}{m}\sum_{t=R+\tau-m}^{R+\tau-1}\widehat{L}_{t+2}=A_{\tau,P}\!+\!B_{P}\!+\!U_{P} \ \mathrm{where} \  \tau=\lambda\cdot P_{2}=\lambda\cdot(T-R-1) \ \mathrm{and} \  \lambda\in[\mu,1].\]
\item I write the $2$-step ahead forecast as: 
\begin{eqnarray}\frac{1}{m}\sum_{j=0}^{m-1}\overrightarrow{\mathcal{RL}}_{R+i+2+j|R+i+j}\!\!\!\!&=&\!\!\!\!A_{2,i,T-R-1}+B_{2,T-R-1}+U_{2,T-R-1} \nonumber \\
&&\ \ \ \ \   \ \ \ \ \   \ \ \ \ \   \mathrm{where}\  i=0,1,\cdots,T-R-2+1-m. \nonumber
\end{eqnarray}
\item The $\tau$ parameter doesn't really affect the last window since $\lambda=1$ for the last window, which means that $\tau=P_{2}$. But given this Barbara's and my last windows are the same.
\begin{itemize}
\item For me, the last window goes from $j=0$ to $j=m-1$ for $i=T-R+2-1-m$, so from $\overrightarrow{\mathcal{RL}}_{T-m+1|T-m-1}$ to $\overrightarrow{\mathcal{RL}}_{T|T-2}$.
\item For Barbara, the $\tau$ for the last window is $\tau=P_{2}$ and it goes from $t=R+\tau-m=R+P_{2}-m$ (the 1st observation) to $t=R+\tau-1=R+P_{2}-1$ (the last observation), so from $\widehat{L}_{R+P_{2}-m+2}$ to $\widehat{L}_{R+P_{2}+1}$. 
\item Since $T=R+P_{2}+1$ these are the same across the two notations.
\end{itemize}
\item We look at the observations for the 1st window.
\begin{itemize}
\item For me, the 1st window goes from $j=0$ to $j=m-1$ for $i=0$, so from $\overrightarrow{\mathcal{RL}}_{R+2|R}$ to $\overrightarrow{\mathcal{RL}}_{R+m+1|R+m-1}$.
\item For Barbara, the $\tau$ for the 1st window is $\tau=\lambda\cdot P_{2}=\mu\cdot P_{2}$ and it goes from $t=R+\tau-m=R+\mu\cdot P_{2}-m$ (the 1st observation) to $t=R+\tau-1=R+\mu\cdot P_{2}-1$ (the last observation), so from $\widehat{L}_{R+\mu\cdot P_{2}-m+2}$ to $\widehat{L}_{R+\mu\cdot P_{2}+1}$. 
\item This means that $R+\mu\cdot P_{2}-m+2=R+2$ and $R+\mu\cdot P_{2}+1=R+m+1$ or $\underline{\mu=m/P_{2}=m/(T-R-1)}$. So this means that the $\mu$ for the $2$-step ahead forecasts will be a bit different from the $\mu$ for the $1$-step ahead forecasts ($m/(T-R-1)$ versus $m/(T-R)$).
\end{itemize}
\item We look at the observations for the 2nd window.
\begin{itemize}
\item For me, the 2nd window goes from $j=0$ to $j=m-1$ for $i=1$, so from $\overrightarrow{\mathcal{RL}}_{R+3|R+1}$ to $\overrightarrow{\mathcal{RL}}_{R+m+2|R+m}$.
\item For Barbara, the $\tau$ for the 2nd window is $\tau=\lambda\cdot P_{2}=\widetilde{\mu}\cdot P_{2}$ and it goes from $t=R+\tau-m=R+\widetilde{\mu}\cdot P_{2}-m$ (the 1st observation) to $t=R+\tau-1=R+\widetilde{\mu}\cdot P_{2}-1$ (the last observation), so from $\widehat{L}_{R+\widetilde{\mu}\cdot P_{2}-m+2}$ to $\widehat{L}_{R+\widetilde{\mu}\cdot P_{2}+1}$. 
\item This means that $R+\widetilde{\mu}\cdot P_{2}-m+2=R+3$ and $R+\widetilde{\mu}\cdot P_{2}+1=R+m+2$ or $\widetilde{\mu}=(m+1)/P_{2}=(m+1)/(T-R-1)$. Again, I don't think this $\widetilde{\mu}$ is really needed for anything.
\end{itemize}
\item So I think Barbara's $\mu$'s for the $2$ step ahead forecasts are $\mu\in [\frac{m+0}{T-R-1},\frac{m+1}{T-R-1},\cdots,$ $\frac{m+T-R-1-m}{T-R-1}]$ or $\mu=\frac{m+i}{T-R-1}$ for $i=0,1,\cdots,T-R-m-1$.  I think that only
the lowest $\mu$ that is relevant for the critical values in Table~1.
\end{itemize}
\item Now consider the $h$-step ahead forecasts.
\begin{itemize}
\item Barbara writes her $h$-step ahead forecast as: 
\[
\frac{1}{m}\sum_{t=R+\tau-m}^{R+\tau-1}\widehat{L}_{t+h}=A_{\tau,P}\!+\!B_{P}\!+\!U_{P} \ \mathrm{where} \  \tau=\lambda\cdot P_{h}=\lambda\cdot(T-R-(h-1)) \ \mathrm{and} \  \lambda\in[\mu,1].\]
\item I write the $h$-step ahead forecast as: 
\begin{eqnarray}
\frac{1}{m}\sum_{j=0}^{m-1}  \overrightarrow{\mathcal{RL}}_{R+i+h+j|R+i+j}
\!\!\!\!&=&\!\!\!\!A_{h,i,T-R-(h-1)}+B_{2,T-R-(h-1)}+U_{2,T-R-(h-1)} \nonumber \\
&&\ \ \ \ \   \ \ \ \ \   \ \ \ \ \   \mathrm{where}\  i=0,1,\cdots,T-R-h+1-m. \nonumber
\end{eqnarray}
\item The $\tau$ parameter doesn't really affect the last window since $\lambda=1$ for the last window, which means that $\tau=P_{h}$. But given this Barbara's and my last windows are the same.
\begin{itemize}
\item For me, the last window goes from $j=0$ to $j=m-1$ for $i=T-R+h-1-m$, so from $\overrightarrow{\mathcal{RL}}_{T-m+1|T-m-(h-1)}$ to $\overrightarrow{\mathcal{RL}}_{T|T-h}$.
\item For Barbara, the $\tau$ for the last window is $\tau=P_{h}$ and it goes from $t=R+\tau-m=R+P_{h}-m$ (the 1st observation) to $t=R+\tau-1=R+P_{h}-1$ (the last observation), so from $\widehat{L}_{R+P_{h}-m+h}$ to $\widehat{L}_{R+P_{h}+(h-1)}$. 
\item Since $T=R+P_{h}+(h-1)$ these are the same across the two notations.
\end{itemize}
\item We look at the observations for the 1st window.
\begin{itemize}
\item For me, the 1st window goes from $j=0$ to $j=m-1$ for $i=0$, so from $\overrightarrow{\mathcal{RL}}_{R+h|R}$ to $\overrightarrow{\mathcal{RL}}_{R+m+(h-1)|R+m-1}$.
\item For Barbara, the $\tau$ for the 1st window is $\tau=\lambda\cdot P_{h}=\mu\cdot P_{h}$ and it goes from $t=R+\tau-m=R+\mu\cdot P_{h}-m$ (the 1st observation) to $t=R+\tau-1=R+\mu\cdot P_{h}-1$ (the last observation), so from $\widehat{L}_{R+\mu\cdot P_{h}-m+h}$ to $\widehat{L}_{R+\mu\cdot P_{h}+(h-1)}$. 
\item This means that $R+\mu\cdot P_{h}-m+h=R+h$ and $R+\mu\cdot P_{h}+(h-1)=R+m+(h-1)$ or $\underline{\mu=m/P_{h}=m/(T-R-(h-1))}$. So this means that the $\mu$ for the $h$-step ahead forecasts will be a bit different from the $\mu$ for the $1$-step ahead forecasts and the $\mu$ for the $2$-step ahead forecast ($m/(T-R-(h-1))$ versus $m/(T-R-1)$ and $m/(T-R)$).
\end{itemize}
\item We look at the observations for the 2nd window.
\begin{itemize}
\item For me, the 2nd window goes from $j=0$ to $j=m-1$ for $i=1$, so from $\overrightarrow{\mathcal{RL}}_{R+h+1|R+1}$ to $\overrightarrow{\mathcal{RL}}_{R+m+h|R+m}$.
\item For Barbara, the $\tau$ for the 1st window is $\tau=\lambda\cdot P_{h}=\widetilde{\mu}\cdot P_{h}$ and it goes from $t=R+\tau-m=R+\widetilde{\mu}\cdot P_{h}-m$ (the 1st observation) to $t=R+\tau-1=R+\widetilde{\mu}\cdot P_{h}-1$ (the last observation), so from $\widehat{L}_{R+\widetilde{\mu}\cdot P_{h}-m+h}$ to $\widehat{L}_{R+\widetilde{\mu}\cdot P_{h}+(h-1)}$. 
\item This means that $R+\widetilde{\mu}\cdot P_{h}-m+h=R+h+1$ and $R+\widetilde{\mu}\cdot P_{h}+(h-1)=R+m+h$ or $\widetilde{\mu}=(m+1)/P_{h}=(m+1)/(T-R-(h-1))$. Again, I don't think this $\widetilde{\mu}$ is really needed for anything.
\end{itemize}
\item So I think Barbara's $\mu$'s for the $h$ step ahead forecasts are $\mu\in [\frac{m+0}{T-R-(h-1)},\frac{m+1}{T-R-(h-1)},$ $\cdots,\frac{m+T-R-(h-1)-m}{T-R-(h-1)}]$ or $\mu=\frac{m+i}{T-R-(h-1)}$ for $i=0,1,\cdots,T-R-m-(h-1)$.  I think that only
the lowest $\mu$ that is relevant for the critical values in Table~1.
\end{itemize}
\item When Barbara does the exchange rate forecasting example the ultimate finding is not so interesting, since basically she just reports that ``the lack of out-of-sample predictive ability is related to the lack of in-sample predictive ability.''
\item We should be able to do something like Barbara's decomposition because we can generate in-sample forecasts (and one only ends up using the in-sample forecast errors for the very last period of the sample) but it is not clear -- indeed very unlikely -- that the components that we decompose the rolling relative out-of-sample into will be independent and that all the proofs that Barbara does will go through.
\end{itemize}

\textbf{Doing PLS with lagged dependent variables}
\begin{itemize}
\item My reading of Groen and Kapetanios (NY Fed Staff Report no. 327, 2009) is that the following is how we would implement PLS with lagged dependent variables.
\item Note that the example given is Groen and Kapetanios sounds like it too is for DAR type forecasts but I would think that this would work with RAR type forecasts too.
\item The model is:
\[ y_{t}=\alpha+\beta^{\prime}_{\mathsf{1}}f^{pls}_{\mathsf{1},t-1}+\beta^{\prime}_{\mathsf{2}}f^{pls}_{\mathsf{2},t-1}+\beta^{\prime}_{\mathsf{3}}f^{pls}_{\mathsf{3},t-1}+\sum_{i=1}^{p}\rho_{i}y_{t-i}+\epsilon_{t}\]
\begin{itemize}
\item I stopped at three factors because all the other models that we have been looking at have been using three factors.
\end{itemize}
\item The first thing we need to do is de-mean the variable that we are interested in forecasting, $y_{t}$, and the large number of variables that we are trying to compress with PLS, that is $x^{s}_{t-1}$, where $v=1,2,\cdots S$.
\begin{itemize}
\item We denote the de-meaned variables by $y^{d}_{t}$ and $x^{d}_{s,t-1}$.
\end{itemize}
\item The way Groen and Kapetanios explain what to do with the lagged dependent variable is that ``one needs to control for the effect of $y_{t-1}$, $y_{t-2}$, $\cdots$, $y_{t-p}$ (or $y^{d}_{t-1}$, $y^{d}_{t-2}$, $\cdots$, $y^{d}_{t-p}$, though I don't think it makes a difference) on the covariances between $y^{d}_{t}$ and $x^{d}_{s,t-1}$.'' 
\begin{itemize}
\item Doing this -- which is done in the next bullet point -- is basically the Frish-Waugh~(1933) theorem (see Greene page 180).  This says that if you have a model $f_{t}=\kappa\cdot g_{t}+\lambda\cdot h_{t}+\epsilon_{t}$ you will retrieve the same estimate of $\kappa$ from estimating this equation as you would by (i)~regressing $f_{t}$ on $h_{t}$ and saving $\widehat{\nu}^{f}_{t}=f_{t}-\widehat{\theta}^{f}\cdot h_{t}$,
(ii)~regressing $g_{t}$ on $h_{t}$ and saving $\widehat{\nu}^{g}_{t}=g_{t}-\widehat{\theta}^{g}\cdot h_{t}$, and (iii)~then regressing $\widehat{\nu}^{f}_{t}$ on $\widehat{\nu}^{g}_{t}$.  That is, $\widehat{\nu}^{f}_{t}=\kappa\widehat{\nu}^{g}_{t}+\omega_{t}$ would give you the same estimate of $\kappa$. 
\end{itemize}
\item So then one regresses:
\begin{eqnarray}
y^{d}_{t}&=&\sum_{i=1}^{p}\gamma_{i}\cdot y_{t-i}+u_{t} \ \ \ \  \ \ \ \ \textrm{and} \  \textrm{saves} \  
\widehat{u}_{t}=y^{d}_{t}-\sum_{i=1}^{p}\widehat{\gamma}_{i}\cdot y_{t-i} \nonumber \\
x^{d}_{1,t-1}&=&\sum_{i=1}^{p}\delta_{1,i}\cdot y_{t-i}+v_{1,t-1} \ \ \textrm{and} \  \textrm{saves} \  
\widehat{v}_{1,t-1}=x^{d}_{1,t-1}-\sum_{i=1}^{p}\widehat{\delta}_{1,i}\cdot y_{t-i}  \nonumber \\
x^{d}_{2,t-1}&=&\sum_{i=1}^{p}\delta_{2,i}\cdot y_{t-i}+v_{2,t-1} \ \ \textrm{and} \  \textrm{saves} \  
\widehat{v}_{2,t-1}=x^{d}_{2,t-1}-\sum_{i=1}^{p}\widehat{\delta}_{2,i}\cdot y_{t-i}  \nonumber \\
&&\cdots  \nonumber \\
&&\cdots  \nonumber \\
x^{d}_{S,t-1}&=&\sum_{i=1}^{p}\delta_{S,i}\cdot y_{t-i}+v_{S,t-1} \ \ \textrm{and} \  \textrm{saves} \  
\widehat{v}_{S,t-1}=x^{d}_{S,t-1}-\sum_{i=1}^{p}\widehat{\delta}_{S,i}\cdot y_{t-i} \nonumber 
\end{eqnarray}
\item So then what I think one wants to do the PLS on is on $\widehat{u}_{t}$ and the large number of variables $\widehat{v}_{1,t-1}$, $\widehat{v}_{2,t-1}$, $\cdots$, $\widehat{v}_{S,t-1}$ and I think one can follow the algorithm that Groen and Kapetanios describe for doing this.
\begin{itemize}
\item The variables we start with are $\widehat{u}_{t}$ and the large number of variables $\widehat{v}_{1,t-1}$, $\widehat{v}_{2,t-1}$, $\cdots$, $\widehat{v}_{S,t-1}$.
\item Determine the $S\times1$ vector of indicator variables or loadings $w_{\mathsf{1}}=\left(w_{\mathsf{1},1}, w_{\mathsf{1},2},\cdots,w_{\mathsf{1},S}\right)^{\prime}$ for the \underline{first} PLS factor by computing individual covariances $w_{\mathsf{1},s}=Cov(\widehat{u}_{t},\widehat{v}_{s,t-1})$, for $s=1,2,\cdots,S$. 
\begin{itemize}
\item Because the $\widehat{v}_{s,t-1}$ variables have unit variance $w_{\mathsf{1},s}=Cov(\widehat{u}_{t},\widehat{v}_{s,t-1})/Var(\widehat{v}_{s,t-1})$, which means that we could calculate each $w_{\mathsf{1},s}$ by regressing $\widehat{u}_{t}$ on each $\widehat{v}_{s,t-1}$ and saving the regression coefficient.  This sort of gives this analogy to simple model averaging.
\end{itemize}
\item Construct the \underline{first} PLS factor as:
\[ f_{\mathsf{1},t-1}=\sum_{s=1}^{S}w_{\mathsf{1},v}\cdot\widehat{v}_{s,t-1}=\sum_{s=1}^{S}Cov(\widehat{u}_{t},\widehat{v}_{s,t-1})\cdot\widehat{v}_{s,t-1}. \]
\item For the next PLS factor we want to think about the part of $\widehat{u}_{t}$ and the $\widehat{v}_{s,t-1}$ terms that is orthogonal to the first factor $f_{\mathsf{1},t}$ so one regresses:
\begin{eqnarray}
\widehat{u}_{t}&=&\gamma_{\mathsf{1}}\cdot f_{\mathsf{1},t-1}+\widehat{\widehat{u}}_{t} \ \ \ \ \ \ \   \ \textrm{and} \  \textrm{saves} \  
\widehat{\widehat{u}}_{t}=\widehat{u}_{t}-\widehat{\gamma}_{\mathsf{1}}\cdot f_{\mathsf{1},t-1} \nonumber \\
\widehat{v}_{1,t-1}&=&\delta_{\mathsf{1},1}\cdot f_{\mathsf{1},t-1}+\widehat{\widehat{v}}_{1,t-1} \ \ \textrm{and} \  \textrm{saves} \  
\widehat{\widehat{v}}_{1,t-1}=\widehat{v}_{1,t-1}-\widehat{\delta}_{\mathsf{1},1}\cdot f_{\mathsf{1},t-1}  \nonumber \\
\widehat{v}_{2,t-1}&=&\delta_{\mathsf{1},2}\cdot f_{\mathsf{1},t-1}+\widehat{\widehat{v}}_{2,t-1} \ \ \textrm{and} \  \textrm{saves} \  
\widehat{\widehat{v}}_{2,t-1}=\widehat{v}_{2,t-1}-\widehat{\delta}_{\mathsf{1},2}\cdot f_{\mathsf{1},t-1}  \nonumber \\
&&\cdots  \nonumber \\
&&\cdots  \nonumber \\
\widehat{v}_{S,t-1}&=&\delta_{\mathsf{1},S}\cdot f_{\mathsf{1},t-1}+\widehat{\widehat{v}}_{S,t-1} \ \ \textrm{and} \  \textrm{saves} \  
\widehat{\widehat{v}}_{S,t-1}=\widehat{v}_{S,t-1}-\widehat{\delta}_{\mathsf{1},S}\cdot f_{\mathsf{1},t-1} \nonumber 
\end{eqnarray}
\item Then the variables we work with are $\widehat{\widehat{u}}_{t}$ and the large number of variables $\widehat{\widehat{v}}_{1,t-1}$, $\widehat{\widehat{v}}_{2,t-1}$, $\cdots$, $\widehat{\widehat{v}}_{S,t-1}$.
\item Determine the $S\times1$ vector of indicator variables or loadings $w_{\mathsf{2}}=\left(w_{\mathsf{2},1}, w_{\mathsf{2},2},\cdots,w_{\mathsf{2},S}\right)^{\prime}$ for the \underline{second} PLS factor by computing individual covariances $w_{\mathsf{2},s}=Cov(\widehat{\widehat{u}}_{t},\widehat{\widehat{v}}_{s,t-1})$, for $s=1,2,\cdots,S$.
\begin{itemize}
\item Because the $\widehat{\widehat{v}}_{s,t-1}$ variables have unit variance $w_{\mathsf{1},s}=Cov(\widehat{\widehat{u}}_{t},\widehat{\widehat{v}}_{s,t-1})/Var(\widehat{\widehat{v}}_{s,t-1})$, which means that we could calculate each $w_{\mathsf{2},s}$ by regressing $\widehat{\widehat{u}}_{t}$ on each $\widehat{\widehat{v}_{s,t-1}}$ and saving the regression coefficient.  Simple model averaging doesn't really get to this point.  The analogy is really just there for the first factor.
\end{itemize}
\item Construct the \underline{second} PLS factor as:
\[ f_{\mathsf{2},t-1}=\sum_{s=1}^{S}w_{\mathsf{2},s}\cdot\widehat{\widehat{v}}_{s,t-1}=\sum_{s=1}^{S}Cov(\widehat{\widehat{u}}_{t},\widehat{\widehat{v}}_{s,t-1})\cdot\widehat{\widehat{v}}_{s,t-1}. \]
\item For the next PLS factor we want to think about the part of $\widetilde{\overline{y}}^{d}_{t+1}$ and the $\widetilde{\overline{x}}^{d}_{v,t}$ terms that is orthogonal to the first factor $f_{I,t}$ so one regresses:
\begin{eqnarray}
\widehat{\widehat{u}}_{t}&=&\gamma_{\mathsf{2}}\cdot f_{\mathsf{2},t-1}+\widehat{\widehat{\widehat{u}}}_{t} \ \ \ \ \ \ \  \ \textrm{and} \  \textrm{saves} \  
\widehat{\widehat{\widehat{u}}}_{t}=\widehat{\widehat{u}}_{t}-\widehat{\gamma}_{\mathsf{2}}\cdot f_{\mathsf{2},t-1} \nonumber \\
\widehat{\widehat{v}}_{1,t-1}&=&\delta_{\mathsf{2},1}\cdot f_{\mathsf{2},t-1}+\widehat{\widehat{\widehat{v}}}_{1,t-1} \ \ \textrm{and} \  \textrm{saves} \  
\widehat{\widehat{\widehat{v}}}_{1,t-1}=\widehat{\widehat{v}}_{1,t-1}-\widehat{\delta}_{\mathsf{2},1}\cdot f_{\mathsf{2},t-1}  \nonumber \\
\widehat{\widehat{v}}_{2,t-1}&=&\delta_{\mathsf{2},2}\cdot f_{\mathsf{2},t-1}+\widehat{\widehat{\widehat{v}}}_{2,t-1} \ \ \textrm{and} \  \textrm{saves} \  
\widehat{\widehat{\widehat{v}}}_{2,t-1}=\widehat{\widehat{v}}_{2,t-1}-\widehat{\delta}_{\mathsf{2},2}\cdot f_{\mathsf{2},t-1}  \nonumber \\
&&\cdots  \nonumber \\
&&\cdots  \nonumber \\
\widehat{\widehat{v}}_{S,t-1}&=&\delta_{\mathsf{2},S}\cdot f_{\mathsf{2},t-1}+\widehat{\widehat{\widehat{v}}}_{S,t-1} \ \ \textrm{and} \  \textrm{saves} \  
\widehat{\widehat{\widehat{v}}}_{S,t-1}=\widehat{\widehat{v}}_{S,t-1}-\widehat{\delta}_{\mathsf{2},S}\cdot f_{\mathsf{2},t-1} \nonumber 
\end{eqnarray}
\item Then the variables we work with are $\widehat{\widehat{\widehat{u}}}_{t}$ and the large number of variables $\widehat{\widehat{\widehat{v}}}_{1,t-1}$, $\widehat{\widehat{\widehat{v}}}_{2,t-1}$, $\cdots$, $\widehat{\widehat{\widehat{v}}}_{S,t-1}$.
\item Determine the $S\times1$ vector of indicator variables or loadings $w_{\mathsf{3}}=\left(w_{\mathsf{3},1}, w_{\mathsf{3},2},\cdots,w_{\mathsf{3},S}\right)^{\prime}$ for the \underline{third} PLS factor by computing individual covariances $w_{\mathsf{3},s}=Cov(\widehat{\widehat{\widehat{u}}}_{t},\widehat{\widehat{\widehat{v}}}_{s,t-1})$, for $s=1,2,\cdots,S$.
\begin{itemize}
\item Because the $\widehat{\widehat{\widehat{v}}}_{s,t-1}$ variables have unit variance $w_{\mathsf{1},s}=Cov(\widehat{\widehat{\widehat{u}}}_{t},\widehat{\widehat{\widehat{v}}}_{s,t-1})/Var(\widehat{\widehat{\widehat{v}}}_{s,t-1})$, which means that we could calculate each $w_{\mathsf{2},s}$ by regressing $\widehat{\widehat{\widehat{u}}}_{t}$ on each $\widehat{\widehat{\widehat{v}}}_{s,t-1}$ and saving the regression coefficient.  Again, simple model averaging doesn't really get to this point.  The analogy is really just there for the first factor.
\end{itemize}
\item Construct the \underline{third} PLS factor as:
\[ f_{\mathsf{3},t-1}=\sum_{s=1}^{S}w_{\mathsf{3},s}\cdot\widehat{\widehat{\widehat{v}}}_{s,t-1}=\sum_{s=1}^{S}Cov(\widehat{\widehat{\widehat{u}}}_{t},\widehat{\widehat{\widehat{v}}}_{s,t-1})\cdot\widehat{\widehat{\widehat{v}}}_{s,t-1}. \]
\end{itemize}
\item Note that the example given is Groen and Kapetanios sounds like it too is for DAR type forecasts but I would think that this would work with RAR type forecasts too.
\item Then I think one can just go ahead and estimate the model below (which is the same as the one given at the begining of this section):
\[ y_{t}=\alpha+\beta^{\prime}_{\mathsf{1}}f^{pls}_{\mathsf{1},t-1}+\beta^{\prime}_{\mathsf{2}}f^{pls}_{\mathsf{2},t-1}+\beta^{\prime}_{\mathsf{3}}f^{pls}_{\mathsf{3},t-1}+\sum_{i=1}^{p}\rho_{i}y_{t-i}+\epsilon_{t}\]
\item Without lagged dependent variables the above model with just the first PLS factor would be the same as the simple model averaging approach.  I think it would still be the same with just the first PLS factor and the lagged dependent variables.  (Presumably we could check this.)  I think having the additional factors goes beyond the simple model averaging approach.
\end{itemize}
%\begin{itemize}
%\item 
%\end{itemize}


\textbf{Panel models}
\begin{itemize}
\item  Possible models: 
\begin{itemize}
\item Panel model
\item Bank-specific equations with SUR
\item Swamy random coefficients model?
\end{itemize}
\item Evaluate forecasts but also compare them to the SNL analysts forecasts.
\end{itemize}
\end{document}               % End of document.
